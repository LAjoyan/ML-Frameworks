{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb00e381",
   "metadata": {},
   "source": [
    "## Lektion 5 - Scikit-learn API (Del 2)\n",
    "Assignment: Unsupervised learning with K-Means and PCA\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Work with a numeric dataset\n",
    "- Visualize results\n",
    "- Add short comments to explain your plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f4a38",
   "metadata": {},
   "source": [
    "## Task 1: K-Means clustering\n",
    "Cluster a numeric dataset and choose k using the elbow method.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load a numeric dataset (iris features or another)\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Reading in data is usually about this simple\n",
    "# It often looks like: data = pd.read_csv(\"filename.csv\")\n",
    "\n",
    "data = load_iris(as_frame=True)\n",
    "\n",
    "# It is not guaranteed that your input data is a neat dictionary\n",
    "# split into data and target, BUT: regardless of how the data is\n",
    "# structured, we want to get an X and a y. \n",
    "X = data.data\n",
    "# For unsupervised learning, it may be that\n",
    "# we don’t even have a y.\n",
    "# The task may be to find groupings (clusters).\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8a655",
   "metadata": {},
   "source": [
    "## Extra EDA-step, wi plot the data\n",
    "A common mistake: We do EDA outside the notebook, online, or by googling around. When we then come back to the notebook,\n",
    "we have forgotten what we did and why. That’s why it’s important to do EDA in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn pairplot exempel\n",
    "# Seaborn is a good complement to matplotlib.\n",
    "# A very common use case is pairplot.\n",
    "# Pairplot plots high-dimensional data, broken down into\n",
    "# pairs of two dimensions, AS WELL AS the distribution of\n",
    "# all variables (histograms).\n",
    "\n",
    "sns.pairplot(data=data.frame, hue=\"target\", vars=data.feature_names, palette=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77537267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d91bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled[:,1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd86249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X before scaling:\", X)\n",
    "print(\"X after scaling:\", X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08119b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try k = 1..8 and compute inertia\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Try k = 1..8 and compute inertia (elbow)\n",
    "# Intertia: hur lika klustrena är inombords\n",
    "\n",
    "# I loopen nedan så tränar vi KMeans-modeller med flera olika K\n",
    "# för att utvärdera vilket som ger bäst resultat\n",
    "# ALLTSÅ: Hur många kluster vi rimligtvis har i vår data\n",
    "ks = range(1, 9)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_scaled)\n",
    "    inertias.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d190fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the elbow curve and choose k\n",
    "\n",
    "# In the plot, it looks a bit like we have an elbow at k=2?\n",
    "# But maybe also a small one at k=3?\n",
    "\n",
    "# For our data, this seems reasonable. To our eyes,\n",
    "# it looks like the data could maybe be 2 clusters, and maybe 3.\n",
    "# In practice, we might try clustering both.\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(list(ks), inertias, marker=\"o\")\n",
    "plt.title(\"Elbow curve for K-Means\")\n",
    "plt.xlabel(\"k (number of clusters)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60000e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw in the elbow plot that k=3 was probably the best!\n",
    "\n",
    "# Choose k=3 (reasonable for Iris) and fit\n",
    "k_best = 3\n",
    "kmeans = KMeans(n_clusters=k_best, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Ramverk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
