{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3a1ea6",
   "metadata": {},
   "source": [
    "### Lecture 7 - Automatisk differentiering och optimering\n",
    "Assignment: Gradients and optimizers\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Use PyTorch or JAX\n",
    "- Keep examples small and explain with comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0954783",
   "metadata": {},
   "source": [
    "## Task 1: Autodiff basics\n",
    "Use automatic differentiation and compare with an analytic derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720032bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define f(x) = x**3 + 2*x\n",
    "def f(x):\n",
    "    return (x**3 + 2*x)\n",
    "\n",
    "print(f(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3094ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodiff df/dx at x=3: 29.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Use autodiff to compute df/dx at x=3\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "# f = x**3 + 2 * x\n",
    "g = f(x)\n",
    "g.backward()\n",
    "print(\"Autodiff df/dx at x=3:\", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf048178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compare with the analytic derivative\n",
    "\n",
    "def f_prim(x):\n",
    "    return 3 * x**2 + 2\n",
    "\n",
    "print(f_prim(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872cc5a",
   "metadata": {},
   "source": [
    "### Task 2: Optimizer comparison\n",
    "Train a small model and compare optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a88c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a small model (e.g., logistic regression)\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# We start by creating a small synthetic classification dataset. \n",
    "# For that, we use sklearn’s make_classification\n",
    "X, y = make_classification(\n",
    "    n_samples=500, n_features=6, n_informative=4, n_redundant=0, random_state=42\n",
    ")\n",
    "\n",
    "# We do not want to do scaling before splitting the data into train/test.\n",
    "# WHY?: Information from the test set will leak into the training set.\n",
    "# This is called data leakage; the model will become artificially good!\n",
    "# But it will generalize poorly\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Here we would typically scale the data!\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea005f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare SGD vs Adam for 20-50 epochs\n",
    "\n",
    "def train_with_optimizer(optimizer_cls, lr=0.01, epochs=30):\n",
    "    model = nn.Sequential(nn.Linear(X_train.shape[1], 1), nn.Sigmoid())\n",
    "    loss_fn = nn.BCELoss() # Binary cross-entropy because there are only 2 classes; with more classes, we use categorical cross-entropy\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=lr)\n",
    "    \n",
    "# In the loop below, we use _ instead of a variable (e.g., i).\n",
    "# That way, we don’t store the value → we save a bit of memory.\n",
    "# In ML/DL, memory is often a limiting factor.\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        preds = model(X_train_t)\n",
    "        loss = loss_fn(preds, y_train_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds = (model(X_test_t) > 0.5).float()\n",
    "        acc = (test_preds == y_test_t).float().mean().item()\n",
    "        test_loss = loss_fn(model(X_test_t), y_test_t).item()\n",
    "    return test_loss, acc\n",
    "\n",
    "sgd_loss, sgd_acc = train_with_optimizer(optim.SGD, lr=0.1, epochs=40)\n",
    "adam_loss, adam_acc = train_with_optimizer(optim.Adam, lr=0.01, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73b599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD -> loss: 0.6475, acc: 0.6300\n",
      "Adam -> loss: 0.6635, acc: 0.6200\n"
     ]
    }
   ],
   "source": [
    "# TODO: Record final loss and accuracy\n",
    "\n",
    "# How do we interpret the results?\n",
    "# Accuracy: What proportion did we get correct? (0–100%, a value between 0 and 1)\n",
    "# Loss: How much error did the model have? (0–∞, lower is better, good for comparing different models on the same problem)\n",
    "print(f\"SGD -> loss: {sgd_loss:.4f}, acc: {sgd_acc:.4f}\")\n",
    "print(f\"Adam -> loss: {adam_loss:.4f}, acc: {adam_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Ramverk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
