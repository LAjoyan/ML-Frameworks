{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84db88df",
   "metadata": {},
   "source": [
    "### Lecture 8 - ML pipelines: Struktur and automatisering\n",
    "Assignment: Build a minimal training pipeline\n",
    "\n",
    "Instructions:\n",
    "\n",
    "* Create a small end-to-end pipeline in code\n",
    "* Save outputs and metrics\n",
    "* Keep short comments explaining design choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b8208",
   "metadata": {},
   "source": [
    "## Task 1: Pipeline in code\n",
    "Build a small end-to-end pipeline with preprocessing and a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a scikit-learn Pipeline with:\n",
    "# - StandardScaler\n",
    "# - Model of choice (LogisticRegression or SVC)\n",
    "\n",
    "# A pipeline is a series of steps that we run\n",
    "# In ML, we often use pipelines for preprocessing steps\n",
    "# such as standardization, transformation, and reshaping,\n",
    "# but also for training.\n",
    "\n",
    "# Today, we build a pipeline with standardization and model creation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Below, we build a pipeline.\n",
    "# The pipeline we have built puts together both our preprocessing step\n",
    "# and the creation of our model into one run.\n",
    "# This makes it very easy to recreate the same workflow.\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps = [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=1000))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916abf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate on a dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "data = load_iris(as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed020eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (38, 4)\n",
      "X_train shape: (112, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08e1ee",
   "metadata": {},
   "source": [
    "See L4_assignment_CLASSROOM.ipynb for detailed EDA:\n",
    "\n",
    "[04_scikit_learn_api.ipynb](../Code_alongs/04_scikit_learn_api.ipynb)\n",
    "\n",
    "[Web Version](https://github.com/LAjoyan/ML-Frameworks/blob/main/Code_alongs/04_scikit_learn_api.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553de040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 1.0, 'f1_macro': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0, 'f1_macro': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training:\n",
    "\n",
    "\n",
    "# With our pipeline, we train and then evaluate our model\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, preds),\n",
    "    \"f1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
    "}\n",
    "\n",
    "print(metrics)\n",
    "{'accuracy': 1.0, 'f1_macro': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779185ca",
   "metadata": {},
   "source": [
    "## Task 2: Automate training\n",
    "Wrap the workflow into a reusable experiment function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wrap training in a function run_experiment(config)\n",
    "\n",
    "# A function like this risks becoming very long\n",
    "# We have to walk a fine line between generalizability\n",
    "# and brevity/readability.\n",
    "# If the idea is to be able to reuse our experiment function,\n",
    "# then a long length is a pill we may have to swallow\n",
    "\n",
    "def run_experiment(config = {\"scaler\": StandardScaler(), \"model\": LogisticRegression(max_iter=1000), \"params\": None}):\n",
    "    \n",
    "# Below is a generalizable pipeline\n",
    "# BUT, it may not be maximally useful,\n",
    "# since the user is forced to keep track of\n",
    "# and pass in all the separate steps themselves\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps= [\n",
    "            (\"scaler\", config[\"scaler\"]),\n",
    "            (\"model\", config[\"model\"]),\n",
    "            (\"params\", config[\"params\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def run_experiment_not_generalised(config):\n",
    "    # Use a small dataset to keep runtime low\n",
    "    iris = load_iris()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        iris.data, iris.target, test_size=0.2, random_state=42, stratify=iris.target\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", LogisticRegression(max_iter=config[\"max_iter\"], C=config[\"C\"])),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, preds),\n",
    "        \"f1_macro\": f1_score(y_test, preds, average=\"macro\"),\n",
    "        \"params\": {\"C\": config[\"C\"], \"max_iter\": config[\"max_iter\"]},\n",
    "    }\n",
    "\n",
    "    # Save outputs\n",
    "    (\"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "    joblib.dump(pipeline, \"model.joblib\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "def run_experiment(config):\n",
    "    \"\"\"\n",
    "    Run a general ML experiment entirely from a config dict.\n",
    "\n",
    "    config: dict\n",
    "        {\n",
    "            \"X\": feature matrix,\n",
    "            \"y\": target vector,\n",
    "            \"test_size\": float (optional, default=0.2),\n",
    "            \"random_state\": int (optional, default=42),\n",
    "            \"preprocessing\": list of (name, transformer) tuples (optional),\n",
    "            \"model\": sklearn estimator,\n",
    "            \"params\": dict of hyperparameters for GridSearchCV (optional),\n",
    "            \"scoring\": str or callable metric (optional, default='accuracy')\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract from config with defaults\n",
    "# We fetch the relevant values from the config\n",
    "    X = config[\"X\"]\n",
    "    y = config[\"y\"]\n",
    "    test_size = config.get(\"test_size\", 0.2)\n",
    "    random_state = config.get(\"random_state\", 42)\n",
    "    preprocessing = config.get(\"preprocessing\", [])\n",
    "    model = config[\"model\"]\n",
    "    params = config.get(\"params\", None)\n",
    "    scoring = config.get(\"scoring\", \"accuracy\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Build pipeline\n",
    "    # Below, we build a list of steps\n",
    "    # first is all preprocessing (scaling, transforms, etc.)\n",
    "    # and last is the model.\n",
    "\n",
    "    steps = preprocessing + [(\"model\", model)] \n",
    "\n",
    "# Steps typically look something like this\n",
    "#    steps= [\n",
    "#             (\"scaler\", config[\"scaler\"]),\n",
    "#             (\"model\", config[\"model\"]), \n",
    "#        ]\n",
    "\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # Wrap with GridSearchCV if params provided\n",
    "# ADVANCED: take a look yourselves if youâ€™re curious\n",
    "    if params:\n",
    "        pipeline = GridSearchCV(pipeline, params, cv=5, n_jobs=-1, scoring=scoring)\n",
    "    \n",
    "    # Fit\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    # Here we use sklearn's get_scorer() to calculate our performance\n",
    "# Previously, we usually imported a specific metric (f1-score, accuracy)\n",
    "# We also usually run a confusion matrix; we can do that later\n",
    "\n",
    "    scorer = get_scorer(scoring)\n",
    "    score = scorer(pipeline, X_test, y_test)\n",
    "    \n",
    "    print(f\"{scoring} on test set: {score:.4f}\")\n",
    "    \n",
    "    return pipeline, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d1f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.8421\n",
      "accuracy on test set: 0.9778\n",
      "accuracy on test set: 0.9667\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save metrics to metrics.json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "configs = []\n",
    "config_LogReg_025 = {\n",
    "    \"config_name\": \"LogReg_025\",\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"test_size\": 0.25,\n",
    "    \"random_state\": 123,\n",
    "    \"preprocessing\": [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", PCA(n_components=2))\n",
    "    ],\n",
    "    \"model\": LogisticRegression(max_iter=1000),\n",
    "    \"params\": {\n",
    "        \"model__C\": [0.1, 1, 10],\n",
    "    },\n",
    "    \"scoring\": \"accuracy\"\n",
    "}\n",
    "\n",
    "config_SVM_03 = {\n",
    "    \"config_name\": \"SVM_03\",\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"test_size\": 0.3,\n",
    "    \"random_state\": 456,\n",
    "    \"preprocessing\": [\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ],\n",
    "    \"model\": SVC(),\n",
    "    \"params\": {\n",
    "        \"model__C\": [0.1, 1, 10],\n",
    "        \"model__kernel\": [\"linear\", \"rbf\"]\n",
    "    },\n",
    "    \"scoring\": \"accuracy\"\n",
    "}\n",
    "\n",
    "# create a config for a third experiment with different preprocessing and model\n",
    "config_LogReg_02 = {\n",
    "    \"config_name\": \"LogReg_02\",\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"test_size\": 0.2,\n",
    "    \"random_state\": 789,\n",
    "    \"preprocessing\": [\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ],\n",
    "    \"model\": LogisticRegression(max_iter=1000),\n",
    "    \"params\": {\n",
    "        \"model__C\": [0.01, 0.1, 1],\n",
    "    },\n",
    "    \"scoring\": \"accuracy\"\n",
    "}\n",
    "\n",
    "configs = [config_LogReg_025, config_SVM_03, config_LogReg_02]\n",
    "\n",
    "for config in configs:\n",
    "\n",
    "    pipeline, score = run_experiment(config)\n",
    "\n",
    "    # Save outputs\n",
    "    metrics = {\n",
    "        \"accuracy\": score,\n",
    "        \"params\": pipeline.best_params_ if hasattr(pipeline, \"best_params_\") else None\n",
    "    }\n",
    "\n",
    "    # save our metrics to a json file\n",
    "    with open(f\"metrics_{config['config_name']}.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2) \n",
    "\n",
    "    # save our model to a joblib file\n",
    "    joblib.dump(pipeline, f\"model_{config['config_name']}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a312480b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Save the trained model with joblib\n",
    "\n",
    "joblib.dump(pipeline, \"model.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Ramverk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
