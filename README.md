ğŸ“˜ Lecture 3 â€“ Introduction to ML Frameworks

In this lecture, I explored the basic mathematical and computational foundations behind modern machine learning frameworks, focusing on NumPy, PyTorch, and Scikit-Learn.

âœ… What I Learned

- Creating and manipulating vectors and matrices using NumPy
- Performing dot products and matrix multiplication
- Computing cosine similarity
- Understanding L2 normalization
- Writing and testing simple mathematical functions
- Using PyTorch in eager execution mode
- Comparing eager vs compiled graph execution
- Loading datasets with Scikit-Learn
- Training a Logistic Regression model
- Understanding convergence warnings and model performance

ğŸ§  Key Concepts

- Linear algebra for machine learning
- Vector similarity measures
- Numerical computation with tensors
- Framework execution modes
- Supervised learning fundamentals
- Model evaluation basics

ğŸ“Š Models

Logistic Regression (Scikit-Learn)

Used to perform basic classification on the Iris dataset.

âš™ï¸ Technical Topics

- NumPy arrays and operations
- PyTorch tensors and performance testing
- Scikit-Learn dataset handling
- Model training and evaluation

ğŸ¯ Goal

Build strong foundations in:

Math â†’ Arrays â†’ Tensors â†’ Models â†’ Evaluation

This lecture prepares the groundwork for advanced topics such as:

- Classical Machine Learning
- Unsupervised Learning
- Deep Learning

-------------------------------------------------------------

# ğŸ“˜ Lecture 4 â€“ Scikit-Learn API (Part 1)

In this lecture, I practiced working with machine learning using Scikit-Learn.

## âœ… What I Learned

- Loading ready datasets (Iris, Diabetes)
- Splitting data with `train_test_split` and `random_state`
- Exploring data (EDA) with matplotlib and seaborn
- Using pairplot and scatter plots
- Training models with `fit()` and `predict()`
- Comparing models using loops

## ğŸ“Š Models

### Classification
- Logistic Regression
- SVC

### Regression
- Linear Regression
- Ridge

## ğŸ“ˆ Evaluation

### Classification Metrics
- Accuracy
- Precision
- Recall
- F1-score
- Confusion Matrix

### Regression Metrics
- MAE
- RÂ²

## ğŸ¯ Goal

Learn the basic ML workflow:

**Load â†’ Explore â†’ Train â†’ Predict â†’ Evaluate â†’ Compare**

-------------------------------------------------------------

# ğŸ“˜ Lecture 5 â€“ Scikit-Learn API (Part 2)

In this lecture, I practiced unsupervised learning using Scikit-Learn.
## âœ… What I Learned

- Understanding K-Means clustering  
- Finding the optimal number of clusters using the Elbow method  
- Visualizing clusters with matplotlib  
- Applying PCA (Principal Component Analysis)  
- Reducing dimensions from 4D to 2D and 3D  
- Comparing K-Means clusters with true labels  

## ğŸ“Š Models & Techniques

### ğŸ”¹ Clustering
- K-Means  
- Elbow Method (WCSS / Inertia)  

### ğŸ”¹ Dimensionality Reduction
- PCA (2D visualization)  
- PCA (3D visualization)  

## ğŸ“ˆ Visualization

- Pairplots for EDA  
- Elbow curve plot  
- 2D PCA scatter plots  
- 3D PCA projection  

## ğŸ¯ Goal

Learn the unsupervised ML workflow:

**Load â†’ Explore â†’ Scale â†’ Cluster â†’ Reduce Dimensions â†’ Visualize â†’ Compare**

-------------------------------------------------------------

ğŸ“˜ Lecture 6 â€“ Introduction to Deep Learning

In this lecture, I practiced building and training a simple neural network using PyTorch and the MNIST dataset.

âœ… What I Learned

- Working with tensors in PyTorch
- Loading and preprocessing the MNIST dataset
- Creating a custom neural network
- Using forward propagation
- Training with backpropagation and optimizers
- Using loss functions for classification
- Running training loops with epochs
- Evaluating model performance
- Visualizing predictions and errors

ğŸ“Š Model

Neural Network (Fully Connected)

- Input: 28 Ã— 28 images (flattened)
- Hidden layers with ReLU activation
- Output: 10 classes (digits 0â€“9)
- Softmax classification

ğŸ“ˆ Evaluation

Classification Metrics

- Accuracy
- Loss (training and validation)
- Confusion Matrix

Visual Analysis

- Sample predictions (correct and wrong)
- Training and validation loss curves

ğŸ¯ Goal

Learn the basic deep learning workflow:

Load â†’ Preprocess â†’ Build â†’ Train â†’ Predict â†’ Evaluate â†’ Visualize

-------------------------------------------------------------

ğŸ“˜ Lecture 7 â€“ Automatic Differentiation & Optimization

In this lecture, I explored how modern deep learning frameworks compute gradients automatically and how different optimizers affect model training.

I focused on understanding automatic differentiation in PyTorch and comparing optimization algorithms on a synthetic classification dataset.

âœ… What I Learned

- Using PyTorch Autograd to compute derivatives
- Understanding computational graphs
- Comparing automatic vs analytical gradients
- Creating synthetic datasets with Scikit-Learn
- Preventing data leakage with proper preprocessing
- Scaling data after train/test split
- Building custom training loops
- Training neural networks with different optimizers
- Evaluating model performance

ğŸ§  Key Concepts

- Automatic differentiation (Autograd)
- Gradient computation
- Backpropagation mechanics
- Optimization algorithms
- Data leakage in ML pipelines
- Reproducible ML experiments

ğŸ“Š Models

Neural Network (Binary Classifier)

Used to classify synthetic data into two classes.

- Input: Feature vectors
- Hidden layers with activation functions
- Output: Binary classification
- Loss: Binary Cross Entropy

âš™ï¸ Technical Topics

- PyTorch tensors with gradients
- requires_grad and backward()
- Optimizers (SGD, Adam, etc.)
- Training and evaluation loops
- Scikit-Learn dataset generation
- StandardScaler usage

ğŸ“ˆ Evaluation

- Accuracy (classification performance)
- Loss (training and validation error)
- Optimizer comparison
- Generalization performance

ğŸ¯ Goal

Learn how models actually learn by:

Gradients â†’ Optimization â†’ Parameter Updates â†’ Convergence â†’ Performance

This lecture builds the foundation for:

- Advanced Deep Learning
- Model Tuning
- Training Optimization
- Research-Level ML Experiments